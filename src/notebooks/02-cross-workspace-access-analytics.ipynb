{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19927b0d",
   "metadata": {},
   "source": [
    "# Cross-Workspace Data Access (Analytics Workspace)\n",
    "\n",
    "**Workspace**: Analytics/Data Science (Secondary)  \n",
    "**Purpose**: Access and analyze data created in the Primary workspace  \n",
    "**Catalog**: `shared_data` (shared across workspaces)\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Accessing Delta tables from another workspace via Unity Catalog\n",
    "- Cross-workspace data sharing without data duplication\n",
    "- Centralized governance and permissions\n",
    "- Analytics and ML on shared data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946616ac",
   "metadata": {},
   "source": [
    "## 1. Verify Unity Catalog Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5312d27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check workspace information\n",
    "workspace_url = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "print(f\"Current Workspace: {workspace_url}\")\n",
    "print(f\"This is the ANALYTICS workspace\\n\")\n",
    "\n",
    "# Verify Unity Catalog is enabled\n",
    "uc_enabled = spark.conf.get(\"spark.databricks.unity_catalog.enabled\", \"false\")\n",
    "print(f\"Unity Catalog Enabled: {uc_enabled}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038b0f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available catalogs\n",
    "print(\"Available Catalogs (accessible from this workspace):\")\n",
    "display(spark.sql(\"SHOW CATALOGS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ae73ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to shared catalog\n",
    "spark.sql(\"USE CATALOG shared_data\")\n",
    "spark.sql(\"USE SCHEMA samples\")\n",
    "\n",
    "print(\"‚úÖ Connected to shared_data.samples\")\n",
    "print(\"   This catalog was created in the PRIMARY workspace\")\n",
    "print(\"   But is accessible here via Unity Catalog!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a710040a",
   "metadata": {},
   "source": [
    "## 2. Explore Available Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff96584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List tables in the shared schema\n",
    "print(\"Tables available in shared_data.samples:\")\n",
    "display(spark.sql(\"SHOW TABLES IN shared_data.samples\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f89a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed information about tables\n",
    "tables = [\"customers\", \"products\", \"transactions\"]\n",
    "\n",
    "for table in tables:\n",
    "    full_table_name = f\"shared_data.samples.{table}\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Table: {full_table_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get table details\n",
    "    df = spark.table(full_table_name)\n",
    "    print(f\"Record Count: {df.count():,}\")\n",
    "    print(f\"\\nSchema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    # Show sample data\n",
    "    print(f\"\\nSample Data (first 3 rows):\")\n",
    "    display(df.limit(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a45d1db",
   "metadata": {},
   "source": [
    "## 3. Query Shared Data - Customer Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca42b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer analysis\n",
    "customers_df = spark.table(\"shared_data.samples.customers\")\n",
    "\n",
    "print(\"Customer Distribution by Country:\")\n",
    "display(\n",
    "    customers_df\n",
    "    .groupBy(\"country\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"customer_count\"),\n",
    "        sum(\"lifetime_value\").alias(\"total_lifetime_value\"),\n",
    "        avg(\"lifetime_value\").alias(\"avg_lifetime_value\")\n",
    "    )\n",
    "    .orderBy(desc(\"total_lifetime_value\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadb35a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top customers by lifetime value\n",
    "print(\"Top 5 Customers by Lifetime Value:\")\n",
    "display(\n",
    "    customers_df\n",
    "    .select(\"customer_name\", \"country\", \"total_purchases\", \"lifetime_value\")\n",
    "    .orderBy(desc(\"lifetime_value\"))\n",
    "    .limit(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f37eea0",
   "metadata": {},
   "source": [
    "## 4. Cross-Table Analysis - Transaction Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c438a634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join customers, transactions, and products\n",
    "from pyspark.sql.functions import col, sum as _sum, count, avg, round as _round\n",
    "\n",
    "customers = spark.table(\"shared_data.samples.customers\")\n",
    "transactions = spark.table(\"shared_data.samples.transactions\")\n",
    "products = spark.table(\"shared_data.samples.products\")\n",
    "\n",
    "# Create comprehensive transaction view\n",
    "transaction_analysis = (\n",
    "    transactions\n",
    "    .join(customers, \"customer_id\")\n",
    "    .join(products, \"product_id\")\n",
    "    .select(\n",
    "        col(\"transaction_id\"),\n",
    "        col(\"customer_name\"),\n",
    "        col(\"country\"),\n",
    "        col(\"product_name\"),\n",
    "        col(\"category\"),\n",
    "        col(\"quantity\"),\n",
    "        col(\"total_amount\"),\n",
    "        col(\"transaction_date\"),\n",
    "        col(\"payment_method\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Complete Transaction Analysis:\")\n",
    "display(transaction_analysis.orderBy(desc(\"transaction_date\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc839e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue by product category\n",
    "print(\"Revenue Analysis by Product Category:\")\n",
    "display(\n",
    "    transaction_analysis\n",
    "    .groupBy(\"category\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"transaction_count\"),\n",
    "        _sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "        _sum(\"quantity\").alias(\"total_units_sold\"),\n",
    "        _round(avg(\"total_amount\"), 2).alias(\"avg_transaction_value\")\n",
    "    )\n",
    "    .orderBy(desc(\"total_revenue\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b90964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payment method preferences\n",
    "print(\"Payment Method Distribution:\")\n",
    "display(\n",
    "    transaction_analysis\n",
    "    .groupBy(\"payment_method\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"transaction_count\"),\n",
    "        _sum(\"total_amount\").alias(\"total_amount\"),\n",
    "        _round(avg(\"total_amount\"), 2).alias(\"avg_amount\")\n",
    "    )\n",
    "    .orderBy(desc(\"transaction_count\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e50b43",
   "metadata": {},
   "source": [
    "## 5. Use the Pre-created View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220ca97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the view created in the Primary workspace\n",
    "print(\"Querying shared_data.samples.customer_transactions view:\")\n",
    "print(\"(This view was created in the PRIMARY workspace)\\n\")\n",
    "\n",
    "customer_trans_view = spark.table(\"shared_data.samples.customer_transactions\")\n",
    "display(customer_trans_view.orderBy(desc(\"transaction_date\")).limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf0c37b",
   "metadata": {},
   "source": [
    "## 6. Create Analytics-Specific Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760706b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a materialized aggregate table for analytics\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE shared_data.samples.customer_summary AS\n",
    "SELECT \n",
    "    c.customer_id,\n",
    "    c.customer_name,\n",
    "    c.country,\n",
    "    c.signup_date,\n",
    "    COUNT(t.transaction_id) as transaction_count,\n",
    "    SUM(t.total_amount) as total_spent,\n",
    "    AVG(t.total_amount) as avg_transaction_value,\n",
    "    MAX(t.transaction_date) as last_purchase_date,\n",
    "    DATEDIFF(CURRENT_DATE(), MAX(t.transaction_date)) as days_since_last_purchase\n",
    "FROM shared_data.samples.customers c\n",
    "LEFT JOIN shared_data.samples.transactions t ON c.customer_id = t.customer_id\n",
    "GROUP BY c.customer_id, c.customer_name, c.country, c.signup_date\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Created analytics table: shared_data.samples.customer_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e382438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the customer summary\n",
    "display(\n",
    "    spark.table(\"shared_data.samples.customer_summary\")\n",
    "    .orderBy(desc(\"total_spent\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0593ca",
   "metadata": {},
   "source": [
    "## 7. Machine Learning Feature Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f561bdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for ML model (customer segmentation)\n",
    "from pyspark.sql.functions import when, datediff, current_date, months_between\n",
    "\n",
    "ml_features = (\n",
    "    spark.table(\"shared_data.samples.customer_summary\")\n",
    "    .withColumn(\n",
    "        \"customer_segment\",\n",
    "        when(col(\"total_spent\") >= 20000, \"Premium\")\n",
    "        .when(col(\"total_spent\") >= 10000, \"Gold\")\n",
    "        .when(col(\"total_spent\") >= 5000, \"Silver\")\n",
    "        .otherwise(\"Bronze\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"recency_score\",\n",
    "        when(col(\"days_since_last_purchase\") <= 30, 5)\n",
    "        .when(col(\"days_since_last_purchase\") <= 60, 4)\n",
    "        .when(col(\"days_since_last_purchase\") <= 90, 3)\n",
    "        .when(col(\"days_since_last_purchase\") <= 180, 2)\n",
    "        .otherwise(1)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"frequency_score\",\n",
    "        when(col(\"transaction_count\") >= 10, 5)\n",
    "        .when(col(\"transaction_count\") >= 7, 4)\n",
    "        .when(col(\"transaction_count\") >= 5, 3)\n",
    "        .when(col(\"transaction_count\") >= 3, 2)\n",
    "        .otherwise(1)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"monetary_score\",\n",
    "        when(col(\"total_spent\") >= 20000, 5)\n",
    "        .when(col(\"total_spent\") >= 10000, 4)\n",
    "        .when(col(\"total_spent\") >= 5000, 3)\n",
    "        .when(col(\"total_spent\") >= 2000, 2)\n",
    "        .otherwise(1)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Customer Segmentation with RFM Scores:\")\n",
    "display(\n",
    "    ml_features\n",
    "    .select(\n",
    "        \"customer_name\",\n",
    "        \"country\",\n",
    "        \"customer_segment\",\n",
    "        \"recency_score\",\n",
    "        \"frequency_score\",\n",
    "        \"monetary_score\",\n",
    "        \"total_spent\",\n",
    "        \"transaction_count\"\n",
    "    )\n",
    "    .orderBy(desc(\"monetary_score\"), desc(\"frequency_score\"), desc(\"recency_score\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d72f315",
   "metadata": {},
   "source": [
    "## 8. Demonstrate Unity Catalog Permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf29217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show current permissions (if you have admin access)\n",
    "try:\n",
    "    print(\"Grants on shared_data catalog:\")\n",
    "    display(spark.sql(\"SHOW GRANTS ON CATALOG shared_data\"))\n",
    "except Exception as e:\n",
    "    print(f\"Note: Unable to show grants (requires admin permissions)\\n{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b2acca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check table history (Delta Lake feature)\n",
    "print(\"Transaction History (Delta Lake Time Travel):\")\n",
    "display(spark.sql(\"DESCRIBE HISTORY shared_data.samples.transactions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec20d634",
   "metadata": {},
   "source": [
    "## 9. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c565b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run data quality checks on shared data\n",
    "def run_data_quality_checks(table_name):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Data Quality Report: {table_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    df = spark.table(table_name)\n",
    "    \n",
    "    # Basic stats\n",
    "    total_rows = df.count()\n",
    "    print(f\"‚úì Total Rows: {total_rows:,}\")\n",
    "    \n",
    "    # Check for nulls in each column\n",
    "    print(f\"\\n‚úì Null Value Check:\")\n",
    "    for col_name in df.columns:\n",
    "        null_count = df.filter(col(col_name).isNull()).count()\n",
    "        if null_count > 0:\n",
    "            print(f\"   ‚ö†Ô∏è  {col_name}: {null_count} nulls ({null_count/total_rows*100:.2f}%)\")\n",
    "        else:\n",
    "            print(f\"   ‚úì {col_name}: No nulls\")\n",
    "    \n",
    "    # Duplicate check (if ID column exists)\n",
    "    id_cols = [c for c in df.columns if 'id' in c.lower()]\n",
    "    if id_cols:\n",
    "        id_col = id_cols[0]\n",
    "        unique_count = df.select(id_col).distinct().count()\n",
    "        print(f\"\\n‚úì Uniqueness Check ({id_col}):\")\n",
    "        if unique_count == total_rows:\n",
    "            print(f\"   ‚úì All {id_col} values are unique\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Found {total_rows - unique_count} duplicate {id_col} values\")\n",
    "\n",
    "# Run checks on all tables\n",
    "for table in [\"customers\", \"products\", \"transactions\"]:\n",
    "    run_data_quality_checks(f\"shared_data.samples.{table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ecda1e",
   "metadata": {},
   "source": [
    "## 10. Summary - Unity Catalog Benefits Demonstrated\n",
    "\n",
    "### ‚úÖ **Cross-Workspace Data Access**\n",
    "- Accessed tables created in **PRIMARY workspace** from **ANALYTICS workspace**\n",
    "- No data duplication or complex data pipelines needed\n",
    "- Single source of truth with centralized governance\n",
    "\n",
    "### ‚úÖ **Centralized Metadata**\n",
    "- Three-level namespace: `catalog.schema.table`\n",
    "- Consistent access across all workspaces\n",
    "- Unified data discovery and lineage\n",
    "\n",
    "### ‚úÖ **Security & Governance**\n",
    "- Fine-grained permissions at catalog, schema, and table level\n",
    "- Row and column-level security (can be configured)\n",
    "- Audit logging of all data access\n",
    "\n",
    "### ‚úÖ **Delta Lake Features**\n",
    "- ACID transactions\n",
    "- Time travel and versioning\n",
    "- Schema evolution\n",
    "- Optimization capabilities\n",
    "\n",
    "### üìä **What We Created**\n",
    "- Queried 3 shared tables: customers, products, transactions\n",
    "- Used pre-created view from Primary workspace\n",
    "- Created new analytics table: customer_summary\n",
    "- Prepared ML features with RFM segmentation\n",
    "- Performed data quality checks\n",
    "\n",
    "### üöÄ **Next Steps**\n",
    "- Set up row-level security\n",
    "- Implement column masking for PII data\n",
    "- Create automated data quality pipelines\n",
    "- Build ML models using shared features\n",
    "- Set up data lineage visualization"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
