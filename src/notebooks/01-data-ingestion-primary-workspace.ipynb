{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d961c229",
   "metadata": {},
   "source": [
    "# Data Ingestion - Create Delta Tables (Primary Workspace)\n",
    "\n",
    "**Workspace**: Data Engineering (Primary)  \n",
    "**Purpose**: Ingest sample data and create Delta tables in Unity Catalog  \n",
    "**Catalog**: `shared_data`  \n",
    "**Schema**: `samples`\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Creating sample datasets\n",
    "- Writing data to Delta tables in Unity Catalog\n",
    "- Setting up data that can be accessed from other workspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47228bfc",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe94810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Display Spark and Delta Lake versions\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Delta Lake Version: {spark.conf.get('spark.databricks.delta.version', 'Not available')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16850947",
   "metadata": {},
   "source": [
    "## 2. Verify Unity Catalog Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb344676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current catalog\n",
    "current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "print(f\"Current Catalog: {current_catalog}\")\n",
    "\n",
    "# List available catalogs\n",
    "print(\"\\nAvailable Catalogs:\")\n",
    "display(spark.sql(\"SHOW CATALOGS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d233f1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working catalog and schema\n",
    "spark.sql(\"USE CATALOG shared_data\")\n",
    "spark.sql(\"USE SCHEMA samples\")\n",
    "\n",
    "print(\"Using catalog: shared_data\")\n",
    "print(\"Using schema: samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebc5c12",
   "metadata": {},
   "source": [
    "## 3. Create Sample Customer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e8947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample customer data\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
    "\n",
    "# Define schema\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"customer_name\", StringType(), False),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"signup_date\", DateType(), True),\n",
    "    StructField(\"total_purchases\", IntegerType(), True),\n",
    "    StructField(\"lifetime_value\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "customer_data = [\n",
    "    (1, \"Alice Johnson\", \"alice@example.com\", \"USA\", datetime(2023, 1, 15).date(), 45, 12500.50),\n",
    "    (2, \"Bob Smith\", \"bob@example.com\", \"Canada\", datetime(2023, 2, 20).date(), 32, 8900.75),\n",
    "    (3, \"Charlie Brown\", \"charlie@example.com\", \"UK\", datetime(2023, 3, 10).date(), 67, 22340.00),\n",
    "    (4, \"Diana Prince\", \"diana@example.com\", \"Australia\", datetime(2023, 4, 5).date(), 89, 31200.25),\n",
    "    (5, \"Eve Wilson\", \"eve@example.com\", \"USA\", datetime(2023, 5, 12).date(), 23, 5600.50),\n",
    "    (6, \"Frank Miller\", \"frank@example.com\", \"Germany\", datetime(2023, 6, 18).date(), 56, 15800.00),\n",
    "    (7, \"Grace Lee\", \"grace@example.com\", \"Singapore\", datetime(2023, 7, 25).date(), 78, 28900.75),\n",
    "    (8, \"Henry Davis\", \"henry@example.com\", \"USA\", datetime(2023, 8, 30).date(), 41, 11200.00),\n",
    "    (9, \"Ivy Chen\", \"ivy@example.com\", \"China\", datetime(2023, 9, 14).date(), 93, 35600.50),\n",
    "    (10, \"Jack Robinson\", \"jack@example.com\", \"Canada\", datetime(2023, 10, 8).date(), 28, 7100.25)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "customers_df = spark.createDataFrame(customer_data, schema=customer_schema)\n",
    "\n",
    "print(f\"Created {customers_df.count()} customer records\")\n",
    "display(customers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a4f3b5",
   "metadata": {},
   "source": [
    "## 4. Write Customer Data to Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d938f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Delta table in Unity Catalog\n",
    "table_name = \"shared_data.samples.customers\"\n",
    "\n",
    "customers_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(table_name)\n",
    "\n",
    "print(f\"âœ… Successfully created Delta table: {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ae1b48",
   "metadata": {},
   "source": [
    "## 5. Create Sample Product Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32adda81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample product data\n",
    "product_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), False),\n",
    "    StructField(\"product_name\", StringType(), False),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"stock_quantity\", IntegerType(), True),\n",
    "    StructField(\"supplier\", StringType(), True)\n",
    "])\n",
    "\n",
    "product_data = [\n",
    "    (101, \"Laptop Pro 15\", \"Electronics\", 1299.99, 45, \"TechCorp\"),\n",
    "    (102, \"Wireless Mouse\", \"Accessories\", 29.99, 230, \"Peripherals Inc\"),\n",
    "    (103, \"USB-C Hub\", \"Accessories\", 49.99, 156, \"ConnectTech\"),\n",
    "    (104, \"Monitor 27-inch\", \"Electronics\", 399.99, 67, \"DisplayMax\"),\n",
    "    (105, \"Keyboard Mechanical\", \"Accessories\", 129.99, 89, \"KeyMasters\"),\n",
    "    (106, \"Webcam HD\", \"Electronics\", 79.99, 134, \"VisionTech\"),\n",
    "    (107, \"Headphones Wireless\", \"Audio\", 199.99, 78, \"SoundWave\"),\n",
    "    (108, \"Desk Lamp LED\", \"Office\", 39.99, 201, \"LightCo\"),\n",
    "    (109, \"External SSD 1TB\", \"Storage\", 149.99, 112, \"DataVault\"),\n",
    "    (110, \"Laptop Stand\", \"Accessories\", 59.99, 176, \"ErgoTech\")\n",
    "]\n",
    "\n",
    "products_df = spark.createDataFrame(product_data, schema=product_schema)\n",
    "\n",
    "print(f\"Created {products_df.count()} product records\")\n",
    "display(products_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d5eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Delta table\n",
    "table_name = \"shared_data.samples.products\"\n",
    "\n",
    "products_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(table_name)\n",
    "\n",
    "print(f\"âœ… Successfully created Delta table: {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6361cf5d",
   "metadata": {},
   "source": [
    "## 6. Create Sample Transaction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f51825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample transaction data\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"transaction_id\", IntegerType(), False),\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"product_id\", IntegerType(), False),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"transaction_date\", DateType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True)\n",
    "])\n",
    "\n",
    "transaction_data = [\n",
    "    (1001, 1, 101, 1, 1299.99, datetime(2024, 1, 15).date(), \"Credit Card\"),\n",
    "    (1002, 1, 102, 2, 59.98, datetime(2024, 1, 16).date(), \"Credit Card\"),\n",
    "    (1003, 2, 104, 1, 399.99, datetime(2024, 1, 20).date(), \"PayPal\"),\n",
    "    (1004, 3, 105, 1, 129.99, datetime(2024, 2, 5).date(), \"Credit Card\"),\n",
    "    (1005, 3, 106, 1, 79.99, datetime(2024, 2, 5).date(), \"Credit Card\"),\n",
    "    (1006, 4, 107, 2, 399.98, datetime(2024, 2, 12).date(), \"Debit Card\"),\n",
    "    (1007, 5, 108, 3, 119.97, datetime(2024, 3, 1).date(), \"PayPal\"),\n",
    "    (1008, 6, 109, 1, 149.99, datetime(2024, 3, 8).date(), \"Credit Card\"),\n",
    "    (1009, 7, 110, 2, 119.98, datetime(2024, 3, 15).date(), \"Credit Card\"),\n",
    "    (1010, 8, 101, 1, 1299.99, datetime(2024, 3, 22).date(), \"Debit Card\"),\n",
    "    (1011, 9, 103, 3, 149.97, datetime(2024, 4, 1).date(), \"PayPal\"),\n",
    "    (1012, 10, 102, 5, 149.95, datetime(2024, 4, 5).date(), \"Credit Card\")\n",
    "]\n",
    "\n",
    "transactions_df = spark.createDataFrame(transaction_data, schema=transaction_schema)\n",
    "\n",
    "print(f\"Created {transactions_df.count()} transaction records\")\n",
    "display(transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df52212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Delta table with partitioning\n",
    "table_name = \"shared_data.samples.transactions\"\n",
    "\n",
    "transactions_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .partitionBy(\"transaction_date\") \\\n",
    "    .saveAsTable(table_name)\n",
    "\n",
    "print(f\"âœ… Successfully created Delta table: {table_name}\")\n",
    "print(\"   Partitioned by: transaction_date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fc7b14",
   "metadata": {},
   "source": [
    "## 7. Verify Created Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b74988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all tables in the schema\n",
    "print(\"Tables in shared_data.samples schema:\")\n",
    "display(spark.sql(\"SHOW TABLES IN shared_data.samples\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25f58ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show table details\n",
    "tables = [\"customers\", \"products\", \"transactions\"]\n",
    "\n",
    "for table in tables:\n",
    "    full_table_name = f\"shared_data.samples.{table}\"\n",
    "    count = spark.table(full_table_name).count()\n",
    "    print(f\"\\nðŸ“Š Table: {full_table_name}\")\n",
    "    print(f\"   Record Count: {count}\")\n",
    "    print(f\"   Schema:\")\n",
    "    spark.table(full_table_name).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9295caa",
   "metadata": {},
   "source": [
    "## 8. Test Unity Catalog Lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9024a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a view that joins tables (demonstrates lineage)\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE VIEW shared_data.samples.customer_transactions AS\n",
    "SELECT \n",
    "    c.customer_id,\n",
    "    c.customer_name,\n",
    "    c.country,\n",
    "    t.transaction_id,\n",
    "    t.product_id,\n",
    "    p.product_name,\n",
    "    p.category,\n",
    "    t.quantity,\n",
    "    t.total_amount,\n",
    "    t.transaction_date,\n",
    "    t.payment_method\n",
    "FROM shared_data.samples.customers c\n",
    "JOIN shared_data.samples.transactions t ON c.customer_id = t.customer_id\n",
    "JOIN shared_data.samples.products p ON t.product_id = p.product_id\n",
    "\"\"\")\n",
    "\n",
    "print(\"âœ… Created view: shared_data.samples.customer_transactions\")\n",
    "print(\"   This view demonstrates Unity Catalog lineage tracking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6f82e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the view\n",
    "display(spark.sql(\"SELECT * FROM shared_data.samples.customer_transactions ORDER BY transaction_date DESC\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862b221e",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "### Created Resources:\n",
    "- âœ… **shared_data.samples.customers** - Customer master data (10 records)\n",
    "- âœ… **shared_data.samples.products** - Product catalog (10 records)\n",
    "- âœ… **shared_data.samples.transactions** - Transaction history (12 records, partitioned)\n",
    "- âœ… **shared_data.samples.customer_transactions** - Joined view\n",
    "\n",
    "### Key Features Demonstrated:\n",
    "- Delta Lake table creation in Unity Catalog\n",
    "- Table partitioning for performance\n",
    "- Cross-table views with lineage tracking\n",
    "- Three-level namespace (catalog.schema.table)\n",
    "\n",
    "### Next Steps:\n",
    "- These tables are now accessible from **any workspace** with the same Unity Catalog metastore\n",
    "- Run the **cross-workspace access notebook** in the Analytics workspace to query this data\n",
    "- Set up fine-grained permissions using Unity Catalog grants"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
